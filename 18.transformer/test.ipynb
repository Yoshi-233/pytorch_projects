{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T13:37:29.042553Z",
     "start_time": "2024-07-30T13:37:27.559729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import os\n",
    "import collections"
   ],
   "id": "97e402b7efcb87eb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "读取数据集",
   "id": "7213e89c39ff0431"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T04:25:14.381836Z",
     "start_time": "2024-07-28T04:25:14.334146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_data_nmt():\n",
    "    \"\"\"Load the English-French dataset.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "data_mnt = read_data_nmt()\n",
    "print(data_mnt[0:75])"
   ],
   "id": "7d11e785436a140e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "数据集预处理",
   "id": "55c809bda90b7174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T04:26:37.749311Z",
     "start_time": "2024-07-28T04:26:36.491817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "pre_data = preprocess_nmt(data_mnt)\n",
    "print(pre_data[0:75])"
   ],
   "id": "93ce8201874e0eac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça al\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T04:41:54.417586Z",
     "start_time": "2024-07-28T04:41:54.398662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(pre_data, 600)\n",
    "print(len(source), len(target))\n",
    "print(source[0:5])\n",
    "print(target[0:5])"
   ],
   "id": "50d43f55226de034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 601\n",
      "[['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?']]\n",
      "[['va', '!'], ['salut', '!'], ['cours', '!'], ['courez', '!'], ['qui', '?']]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义词表",
   "id": "8bbda0e2f531751c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T05:20:35.723802Z",
     "start_time": "2024-07-28T05:20:35.717304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "src_vocab = Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(src_vocab.to_tokens(1))"
   ],
   "id": "bc8b8e36e9c337ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T05:28:59.992506Z",
     "start_time": "2024-07-28T05:28:59.977681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = d2l.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    # print(array[0])\n",
    "    valid_len = d2l.reduce_sum(\n",
    "        d2l.astype(array != vocab['<pad>'], d2l.int32), 1)\n",
    "    return array, valid_len\n",
    "\n",
    "num_steps = 10\n",
    "src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "print(src_array[0], src_valid_len[0])"
   ],
   "id": "444740c4a6098b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59,  2,  4,  5,  5,  5,  5,  5,  5,  5])\n",
      "tensor([188,   0,   4,   5,   5,   5,   5,   5,   5,   5])\n",
      "tensor([59,  2,  4,  5,  5,  5,  5,  5,  5,  5]) tensor(3)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T05:37:40.487778Z",
     "start_time": "2024-07-28T05:37:35.852461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, num_steps = 64, 10\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)"
   ],
   "id": "9f4427e66a576e23",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T05:39:15.529298Z",
     "start_time": "2024-07-28T05:39:15.525072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src, src_valid_len, tgt, tgt_valid_len = next(iter(train_iter))\n",
    "print(src[0], src_valid_len[0])\n",
    "print(tgt[0], tgt_valid_len[0])"
   ],
   "id": "8e1ac15e43d74dd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 81, 144,   2,   4,   5,   5,   5,   5,   5,   5]) tensor(4)\n",
      "tensor([100, 117, 171,   6,  56,   2,   4,   5,   5,   5]) tensor(7)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T05:46:38.463172Z",
     "start_time": "2024-07-28T05:46:38.459879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\" \".join(src_vocab.to_tokens(src[0][:src_valid_len[0] - 1])))\n",
    "print(\" \".join(tgt_vocab.to_tokens(tgt[0][:tgt_valid_len[0] - 1])))"
   ],
   "id": "8f7ca27b38f6d9a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i stood .\n",
      "je me suis <unk> debout .\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:47:30.151298Z",
     "start_time": "2024-07-28T08:47:30.147158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state = ([1, 2, 3], [4, 5, 6])\n",
    "state"
   ],
   "id": "106aedc002444289",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [4, 5, 6])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:48:09.674993Z",
     "start_time": "2024-07-28T08:48:09.671247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "l1, l2 = state\n",
    "l1[0] = 100\n",
    "state"
   ],
   "id": "938a6085aad38f32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([100, 2, 3], [4, 5, 6])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T13:37:49.009252Z",
     "start_time": "2024-07-30T13:37:49.002635Z"
    }
   },
   "cell_type": "code",
   "source": "torch.arange((10), dtype=torch.float32)[None, :]",
   "id": "584477db4e3ada43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "597f9977e0ac6f69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
